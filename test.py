import json
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login

# 0. Hugging Face ë¡œê·¸ì¸ (í† í° ì§ì ‘ ì…ë ¥)
login("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")  # ğŸ” ì‹¤ì œ Hugging Face í† í°ìœ¼ë¡œ êµì²´

# 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

# 2. LLM ë¡œë“œ
model_id = "beomi/KoAlpaca-Polyglot-5.8B"
tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    use_auth_token=True,
    device_map="cpu",
    torch_dtype=torch.float16
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id

# 3. JSON íŒŒì¼ ê²½ë¡œ ìˆ˜ë™ ì§€ì •
filename = "card_benefits.json"  # ë™ì¼ í´ë”ì— íŒŒì¼ ìœ„ì¹˜
with open(filename, "r", encoding="utf-8") as f:
    json_list = json.load(f)

# 4. JSON -> ë¬¸ì¥ ë° ì„ë² ë”©
docs = []
embeddings = []

def convert_to_sentence(card):
    return f"{card['card_company']}ì˜ {card['card_name']} ì¹´ë“œëŠ” '{card['title']}' í˜œíƒìœ¼ë¡œ {card['content']}"

for card in json_list:
    doc = convert_to_sentence(card)
    docs.append(doc)
    emb = embedding_model.encode(doc).tolist()
    embeddings.append(emb)

# 5. ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
def search_context(query, top_k=3):
    query_emb = embedding_model.encode(query).tolist()

    def cosine_sim(a, b):
        dot = sum(x * y for x, y in zip(a, b))
        norm_a = sum(x**2 for x in a) ** 0.5
        norm_b = sum(x**2 for x in b) ** 0.5
        return dot / (norm_a * norm_b + 1e-8)

    scores = [cosine_sim(query_emb, emb) for emb in embeddings]
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
    return "\n".join(docs[i] for i in top_indices)

# 6. ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ìƒì„±
def generate_answer(query, context):
    prompt = f"""ì•ˆë…•í•˜ì„¸ìš”! ì•„ë˜ëŠ” ì¹´ë“œ í˜œíƒì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤:

{context}

ì´ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ê³ ê°ì´ ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ì„ í–ˆìŠµë‹ˆë‹¤:
"{query}"

ìœ„ ì¹´ë“œ í˜œíƒ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ, ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥´ë©´ "í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

ë‹µë³€:"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=100,
        do_sample=True,
        temperature=0.8,
        top_k=40,
        top_p=0.9,
        repetition_penalty=1.2,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id
    )
    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    if "ë‹µë³€:" in decoded:
        return decoded.split("ë‹µë³€:")[1].strip()
    return decoded.strip()

# 7. ì±—ë´‡ ì‹¤í–‰
def run_chatbot():
    print("ğŸ’³ ì¹´ë“œ í˜œíƒ Q&A ì±—ë´‡ì…ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥")
    while True:
        query = input("\nğŸ™‹ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
        if query.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        try:
            context = search_context(query)
        except:
            context = ""
        answer = generate_answer(query, context)
        print("\nğŸ§  ë‹µë³€:\n", answer)

# 8. ì‹¤í–‰
if __name__ == "__main__":
    run_chatbot()

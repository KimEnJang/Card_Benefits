import json
import torch
import chromadb
from google.colab import files
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM

# 1. ì„ë² ë”© ëª¨ë¸
embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

# 2. LLM ë¶ˆëŸ¬ì˜¤ê¸°
model_id = "beomi/KoAlpaca-Polyglot-5.8B"
tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id

# 3. ChromaDB ì„¤ì •
client = chromadb.Client()
collection = client.get_or_create_collection(name="card-benefits")

# 4. JSON ì—…ë¡œë“œ ë° ë²¡í„° ì €ì¥
print("ğŸ“ ì¹´ë“œ í˜œíƒ JSON íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (ì˜ˆ: card_benefits.json)")
uploaded = files.upload()
filename = list(uploaded.keys())[0]

with open(filename, "r", encoding="utf-8") as f:
    json_list = json.load(f)

def convert_to_sentence(card):
    return f"{card['card_company']}ì˜ {card['card_name']} ì¹´ë“œëŠ” '{card['title']}' í˜œíƒìœ¼ë¡œ {card['content']}"

for i, card in enumerate(json_list):
    doc = convert_to_sentence(card)
    embedding = embedding_model.encode(doc).tolist()
    collection.add(
        documents=[doc],
        embeddings=[embedding],
        ids=[f"card_{i}"]
    )

# 5. ê²€ìƒ‰ í•¨ìˆ˜ (context ì„ íƒì ìœ¼ë¡œ ë¶™ì„)
def search_context(query, top_k=3):
    query_embedding = embedding_model.encode(query).tolist()
    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)
    return "\n".join(results["documents"][0])

# 6. í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° ì‘ë‹µ ìƒì„±
def generate_answer(query, context=None):
    if context:
        prompt = f"""### Instruction:
ì•„ë˜ëŠ” ì‹ ìš©ì¹´ë“œ í˜œíƒ ì •ë³´ì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆíˆ ì‘ë‹µí•˜ì„¸ìš”.

ì¹´ë“œ í˜œíƒ ì •ë³´:
{context}

ì§ˆë¬¸: {query}

### Response:"""
    else:
        prompt = f"""### Instruction:
ë‹¹ì‹ ì€ ì‹ ìš©ì¹´ë“œ í˜œíƒì— ëŒ€í•´ ì˜ ì•„ëŠ” ìƒë‹´ì›ì…ë‹ˆë‹¤. ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

### Response:"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=300,
        do_sample=True,
        temperature=0.8,
        top_k=40,
        top_p=0.9,
        repetition_penalty=1.2,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id
    )
    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    if "### Response:" in decoded:
        return decoded.split("### Response:")[1].strip()
    return decoded.strip()

# 7. ì±—ë´‡ ì‹¤í–‰
def run_chatbot():
    print("ğŸ’³ ì¹´ë“œ í˜œíƒ Q&A ì±—ë´‡ì…ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥")
    while True:
        query = input("\nğŸ™‹ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
        if query.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ì±—ë´‡ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        try:
            context = search_context(query)
        except:
            context = None

        answer = generate_answer(query, context)
        print("\nğŸ§  ë‹µë³€:\n", answer)

# 8. ì‹¤í–‰
run_chatbot()

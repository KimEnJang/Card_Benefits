import chromadb
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def run_chatbot():
    # 1. ChromaDB ë¶ˆëŸ¬ì˜¤ê¸°
    client = chromadb.PersistentClient(path="./chroma_storage")
    collection = client.get_or_create_collection(name="card-benefits")

    # 2. ì„ë² ë”© ëª¨ë¸
    embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

    # 3. LLM ë¡œë”© (KULLM 5.8B, 4bit + GPU ë˜ëŠ” CPU í™˜ê²½ ìë™ í• ë‹¹)
    tokenizer = AutoTokenizer.from_pretrained("circulus/kullm-polyglot-5.8b-v2")
    model = AutoModelForCausalLM.from_pretrained(
        "circulus/kullm-polyglot-5.8b-v2",
        torch_dtype=torch.float16,
        device_map="auto",
        load_in_4bit=True
    )

    print("ğŸ§¾ ì¹´ë“œ í˜œíƒ Q&A ì±—ë´‡ì…ë‹ˆë‹¤. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)")

    while True:
        question = input("\nğŸ™‹ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
        if question.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # 4. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        query_embedding = embedding_model.encode(question, normalize_embeddings=True)
        results = collection.query(query_embeddings=[query_embedding], n_results=3)
        retrieved_docs = results["documents"][0] if results["documents"] else []
        context = "\n".join(retrieved_docs) if retrieved_docs else ""

        # 5. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•œ ë§íˆ¬)
        prompt = f"""ë‹¹ì‹ ì€ ì¹´ë“œ í˜œíƒ ì •ë³´ë¥¼ ì•ˆë‚´í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
ì œê³µëœ ì¹´ë“œ í˜œíƒ ì •ë³´ë§Œ ì‚¬ìš©í•´ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ í˜œíƒì— ëŒ€í•œ ì •ë³´ëŠ” ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë§í•˜ì„¸ìš”.

ì¹´ë“œ í˜œíƒ ì •ë³´:
{context}

ì§ˆë¬¸:
{question}

ë‹µë³€:"""

        # 6. í…ìŠ¤íŠ¸ ìƒì„±
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.8,
            top_k=40,
            top_p=0.95,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = response.split("ë‹µë³€:")[-1].strip()

        print(f"\nğŸ¤– ì±—ë´‡ ì‘ë‹µ: {answer}")

if __name__ == "__main__":
    run_chatbot()

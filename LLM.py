import chromadb
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def run_chatbot():
    # 1. ChromaDB ë¡œë”©
    client = chromadb.PersistentClient(path="./chroma_storage")
    collection = client.get_or_create_collection(name="card-benefits")

    # 2. ì„ë² ë”© ëª¨ë¸
    embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

    # 3. ê²½ëŸ‰ LLM
    model_id = "skt/kogpt2-base-v2"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    print("ğŸ§¾ ì¹´ë“œ í˜œíƒ Q&A ì±—ë´‡ì…ë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥")

    # 4. ë¬¸ì„œ ë° ì„ë² ë”© ë¶ˆëŸ¬ì˜¤ê¸°
    results_all = collection.get(include=["documents", "embeddings"])
    docs = results_all["documents"]
    embeddings = results_all["embeddings"]

    # 5. ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
    def search_context(query, top_k=3):
        query_emb = embedding_model.encode(query).tolist()

        def cosine_sim(a, b):
            dot = sum(x * y for x, y in zip(a, b))
            norm_a = sum(x**2 for x in a) ** 0.5
            norm_b = sum(x**2 for x in b) ** 0.5
            return dot / (norm_a * norm_b + 1e-8)

        scores = [cosine_sim(query_emb, emb) for emb in embeddings]
        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)

        # title í‚¤ì›Œë“œ í•„í„°ë§
        title_keywords = ["ê¸°ë³¸ í˜œíƒ", "ì¶”ê°€ í˜œíƒ", "ìš°ëŒ€ ì„œë¹„ìŠ¤"]
        filtered = []
        for i in top_indices:
            doc = docs[i]
            if any(k in query and k in doc for k in title_keywords):
                filtered.append(doc)
            if len(filtered) >= top_k:
                break

        # fallback: ìœ ì‚¬ë„ ìƒìœ„ top_k
        if not filtered:
            filtered = [docs[i] for i in top_indices[:top_k]]

        return "\n".join(filtered).strip()

    # 6. ì±—ë´‡ ì‹¤í–‰ ë£¨í”„
    while True:
        question = input("\nğŸ™‹ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
        if question.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        context = search_context(question)

        # context ì—†ìœ¼ë©´ ì¤‘ë‹¨
        if not context or len(context) < 20:
            print("\nğŸ¤– ì±—ë´‡ ì‘ë‹µ: ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ í˜œíƒì— ëŒ€í•œ ì •ë³´ëŠ” ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

        # 7. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""ë‹¹ì‹ ì€ ì¹´ë“œ í˜œíƒì„ ì•ˆë‚´í•˜ëŠ” ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.

[ì¹´ë“œ í˜œíƒ ì •ë³´]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
"{question}"

[ë‹µë³€ ê·œì¹™]
- ë°˜ë“œì‹œ ìœ„ ì¹´ë“œ í˜œíƒ ì •ë³´ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
- ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í˜œíƒ ì œëª©(ì˜ˆ: 'ê¸°ë³¸ í˜œíƒ')ì— í•´ë‹¹í•˜ëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- ê´€ë ¨ ì—†ëŠ” ì •ë³´, ë°˜ë³µ ë¬¸ì¥, ì™¸ë¶€ ì§€ì‹ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
- ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€ì„ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.
- ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ í˜œíƒì— ëŒ€í•œ ì •ë³´ëŠ” í™•ì¸ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

[ë‹µë³€]
"""

        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.7,
            top_k=40,
            top_p=0.9,
            repetition_penalty=2.0,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = response.split("[ë‹µë³€]")[-1].strip()

        print(f"\nğŸ¤– ì±—ë´‡ ì‘ë‹µ: {answer}")

if __name__ == "__main__":
    run_chatbot()

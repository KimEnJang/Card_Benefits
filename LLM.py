
import json
import chromadb
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# 1. ë””ìŠ¤í¬ ê¸°ë°˜ ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = chromadb.PersistentClient(path="./chroma_storage")
collection = client.get_collection(name="card-benefits")

# 2. ì„ë² ë”© ëª¨ë¸ ë¡œë”©
embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

# 3. KULLM ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained("nlpai-lab/kullm3-7b")
model = AutoModelForCausalLM.from_pretrained(
    "nlpai-lab/kullm3-7b",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_4bit=True
)

# 4. ì±—ë´‡ ë£¨í”„ ì‹œì‘
print("ì¹´ë“œ í˜œíƒ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)")
while True:
    question = input("\nì‚¬ìš©ì ì§ˆë¬¸: ")
    if question.strip().lower() in ["exit", "quit", "ì¢…ë£Œ"]:
        print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    # ì§ˆë¬¸ ì„ë² ë”© í›„ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
    query_embedding = embedding_model.encode(question)
    results = collection.query(query_embeddings=[query_embedding], n_results=5)
    retrieved_docs = results["documents"][0]
    context = "\n".join(retrieved_docs)

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt = f"""### ì§ˆë¬¸:
{question}

### ì¹´ë“œ í˜œíƒ ì •ë³´:
{context}

### ë‹µë³€:"""

    # LLM ì¶”ë¡ 
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True, top_p=0.9)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # ì¶œë ¥
    print("\nğŸ¤– ì±—ë´‡ ì‘ë‹µ:")
    print(response.split("### ë‹µë³€:")[-1].strip())

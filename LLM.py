import chromadb
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def run_chatbot():
    # 1. ChromaDB ë¶ˆëŸ¬ì˜¤ê¸°
    client = chromadb.PersistentClient(path="./chroma_storage")
    collection = client.get_or_create_collection(name="card-benefits")

    # 2. ì„ë² ë”© ëª¨ë¸
    embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

    # 3. ê²½ëŸ‰ LLM
    model_id = "skt/kogpt2-base-v2"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    print("ğŸ§¾ ì¹´ë“œ í˜œíƒ Q&A ì±—ë´‡ì…ë‹ˆë‹¤. (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥)")

    while True:
        question = input("\nğŸ™‹ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
        if question.lower() in ["exit", "quit", "ì¢…ë£Œ"]:
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # 4. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        query_embedding = embedding_model.encode(question, normalize_embeddings=True)
        results = collection.query(query_embeddings=[query_embedding], n_results=3)
        retrieved_docs = results["documents"][0] if results["documents"] else []
        context = "\n".join(retrieved_docs).strip()

        # ğŸ” 5. contextê°€ ì—†ìœ¼ë©´ ë‹µë³€ ê¸ˆì§€
        if not context or len(context) < 20:
            print("\nğŸ¤– ì±—ë´‡ ì‘ë‹µ: ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ í˜œíƒì— ëŒ€í•œ ì •ë³´ëŠ” ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

        # âœ… 6. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì§€ì‹œ ê°•í™”)
        prompt = f"""
ë‹¹ì‹ ì€ ì¹´ë“œ í˜œíƒ ì •ë³´ë¥¼ ì•ˆë‚´í•˜ëŠ” ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ì¹´ë“œ í˜œíƒ ì •ë³´ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•´ì•¼ í•˜ë©°, ë‹¤ë¥¸ ì§€ì‹ì´ë‚˜ ì¶”ì¸¡ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
ë§Œì•½ ì•„ë˜ ì •ë³´ì— ë‹µì´ ì—†ìœ¼ë©´ "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ í˜œíƒì— ëŒ€í•œ ì •ë³´ëŠ” ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
ì˜ë¯¸ ì—†ëŠ” ë°˜ë³µ ë¬¸ì¥, ê°™ì€ ë¬¸ì¥ êµ¬ì¡° ë°˜ë³µ, ëª¨í˜¸í•œ ì •ì˜ëŠ” í”¼í•˜ì„¸ìš”.

[ì¹´ë“œ í˜œíƒ ì •ë³´]
{context}

[ì§ˆë¬¸]
{question}

[ë‹µë³€]
"""

        # 7. í…ìŠ¤íŠ¸ ìƒì„±
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.8,
            top_k=40,
            top_p=0.95,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = response.split("[ë‹µë³€]")[-1].strip()

        print(f"\nğŸ¤– ì±—ë´‡ ì‘ë‹µ: {answer}")

if __name__ == "__main__":
    run_chatbot()
